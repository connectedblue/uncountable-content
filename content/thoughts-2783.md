---
id: 2783
title: "Re: If you believe in “Artificial Intelligence”, take five minutes to ask it about stuff you know well"
date: "2025-12-13T06:00:00"
slug: re-if-you-believe-in-artificial-intelligence-take-five-minutes-to-ask-it-about-stuff-you-know-well
site: "https://thoughts.uncountable.uk"
wp_url: "https://thoughts.uncountable.uk"
root_slug: thoughts
site_name: My Thoughts
featured_media_url: null
featured_media_srcset: null
type: post
category:
  - name: Reply Posts
    slug: reply-posts
    id: 61
tag: []
---


<p>Back in Feb 2025, Mike constructed <a href="https://svpow.com/2025/02/14/if-you-believe-in-artificial-intelligence-take-five-minutes-to-ask-it-about-stuff-you-know-well/">a challenge for AI</a> on a subject he has deep knowledge of:</p>



<blockquote class="wp-block-quote is-style-plain is-layout-flow wp-block-quote-is-layout-flow is-style-plain--1">
<p>Now, ChatGPT got&nbsp;<em>every single part of that</em>&nbsp;wrong. It left Riggs, Paul, Olshevsky and me out completely. It credited Werner Janensch for reassigning a species to a genus whose name was coined 19 years after he died. It pulled poor Ernst Stromer in for some reason. It invented “the paleontologist Michael Janensch”, who is presumably the unholy offspring of Werner Janensch and me. And in all three answers, it insisted that the reassignment was done in 1914, 74 years before the name&nbsp;<em>Giraffatitan</em>&nbsp;was coined.</p>



<p>But the worst part of this is not the errors. It’s not the blithe confidence with which the false facts are recited.</p>
</blockquote>



<p>Very helpfully, Mike set out the correct sequence of events, along with reference material.  He also included the original prompt used.</p>



<p>At the time of writing, it&#8217;s now December 2025, some ten months later and it&#8217;s a good chance to test the improvements in models since then.  I retyped exactly the same prompt into Gemini 3 Deep Research and it provided this <a href="https://docs.google.com/document/d/17vbl26g0g7atvlihLBAlpeH79gpkE-RmArP4apGsNH4/edit?tab=t.0">ten page paper response</a> after a few minutes thinking about the question.</p>



<p>My knowledge on the topic of Sauropods is literally zero, but I was able to follow the timeline in their response and it matched exactly to the one provided in Mike&#8217;s post.  It also provided 31 reference sources, including several authored by Mike, and even a mastodon thread where he raised the concerns with the February response.</p>



<p>As a layman, I&#8217;d say the AI has gone from a D- to a B+ over the last few months.  I can&#8217;t say A because I don&#8217;t know if the other details in the paper are accurate or not, but a definite improvement.  </p>



<p>Of course, all the training this year was not just to improve it&#8217;s knowledge of paleontology.  We can probably expect that it has moved from D- to B+ across a wide range of topics simultaneously.  This is what exponential progress should look like.</p>



<p>I don&#8217;t know if there is a mathematical cap on the volume of plausible inference that can be made from the existing human corpus of knowledge, but with the current rate of progress, we might expect LLMs to hit that within a year or two.</p>



<p>We should use tests like these to benchmark where we are on that journey.  <a href="https://simonwillison.net/">Simon Willison</a> famously uses a pelican riding a bicycle to evaluate LLM progress <a href="https://simonwillison.net/">on his blog</a>, but we should all have our own ground truth to draw our own conclusions.</p>
